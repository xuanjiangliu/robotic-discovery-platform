# Robotic Discovery Platform: Vision System

Welcome to the official repository for the Robotic Discovery Platform System. The current module provides a complete, autonomous MLOps pipeline for training, deploying, and monitoring a real-time computer vision service designed to analyze the physical properties of soft actuators.

### System Architecture

The platform is built on a **Service-Oriented Architecture (SOA)** and leverages modern MLOps tools to create a robust and automated workflow.

* **Internal Communication**: gRPC for high-performance, real-time data streaming.
* **ML Experiment Tracking**: MLflow for logging experiments, versioning models, and managing the model lifecycle.
* **Hardware Interface**: Intel RealSense SDK for 3D camera data.

---

## Project Structure

The repository is organized into distinct packages and directories, each with a specific responsibility.

```
robotic-discovery-platform/
│
├── logs/
│   └── vision_service_metrics.csv      # Performance logs generated by the server for drift detection.
│
├── ml/                                 # (Local Only - Not tracked by Git)
│   ├── configs/                        # Stores configuration files like camera calibration data.
│   ├── datasets/                       # Stores training and validation image datasets.
│   └── mlruns/                         # The backend store for all MLflow experiments and models.
│
├── pkg/
│   ├── protos/                         # Auto-generated gRPC Python files (vision_pb2.py, vision_pb2_grpc.py).
│   ├── camera.py                       # High-level, thread-safe wrapper for the Intel RealSense camera.
│   ├── geometry_utils.py               # Core functions for 3D point cloud and curvature analysis.
│   └── segmentation_model.py           # PyTorch definition of the U-Net model architecture.
│
├── protos/
│   └── vision.proto                    # The gRPC service and message definitions (the "blueprint" for communication).
│
├── reports/
│   └── drift_report.png                # Visual reports generated by the drift detector script.
│
├── scripts/
│   ├── monitoring/
│   │   └── drift_detector.py           # Analyzes service logs to detect model performance drift.
│   ├── 01_calibrate_camera.py          # Interactive script to perform intrinsic camera calibration.
│   ├── 02_collect_segmentation_data.py # Script to collect and auto-label new training data.
│   └── train_segmenter.py              # Trains the segmentation model and registers it with MLflow.
│
├── services/
│   └── vision_analysis/
│       ├── client.py                   # A gRPC client to visualize the live analysis from the server.
│       └── server.py                   # The core gRPC server that runs the vision analysis service.
│
└── workflows/
    └── retraining_pipeline.py          # Automated pipeline to retrain, register, and deploy a new model version.
```

---

## Getting Started: A Step-by-Step Guide

This guide covers the entire workflow, from initial setup to running the autonomous MLOps loop.

### 1. Initial System Setup (One-Time)

These steps only need to be performed once on a new machine.

#### Step 1.1: Prepare the Environment

1.  **Activate Virtual Environment:**
    * On macOS/Linux: `source venv/bin/activate`
    * On Windows: `.\venv\Scripts\activate`
2.  **Install Dependencies:**
    ```bash
    pip install -r requirements.txt
    ```

#### Step 1.2: Calibrate the Camera

This step calculates the camera's intrinsic parameters, which is essential for accurate 3D measurements.

1.  **Run the Calibration Script:**
    ```bash
    python scripts/01_calibrate_camera.py
    ```
2.  **Perform Calibration:**
    * Hold a checkerboard pattern in front of the camera.
    * Press **`c`** to capture frames from various angles (15-20 captures recommended).
    * Press **`q`** to finish. The script saves the calibration data to `ml/configs/calibration_data.npz`.

### 2. Data Collection & Initial Model Training

This is the most critical phase for ensuring high model performance.

#### Step 2.1: Collect High-Quality Training Data

1.  **Run the Data Collection Script:**
    ```bash
    python scripts/02_collect_segmentation_data.py
    ```
2.  **Data Capture Strategy:**
    * Place an actuator on a flat, non-reflective surface.
    * Press **`s`** to start/stop saving image/mask pairs.
    * Vary the lighting, backgrounds, and actuator poses to create a diverse dataset.
    * Press **`q`** to quit.

#### Step 2.2: Train and Register the First Model

1.  **Run Training:**
    ```bash
    python scripts/train_segmenter.py
    ```
2.  **Start the MLflow UI:**
    ```bash
    mlflow ui
    ```
3.  **Promote the Model in MLflow:**
    * Open your browser to **http://127.0.0.1:5000**.
    * Navigate to the **Models** tab and select the `Actuator-Segmenter` model.
    * Click on the latest version and add the **`staging`** alias. This marks the model as ready for deployment.

### 3. Running the Live System

1.  **Start the Server:** In a terminal, run:
    ```bash
    python services/vision_analysis/server.py
    ```
    The server will automatically load the model with the "staging" alias from MLflow.

2.  **Start the Client:** In a *second* terminal, run:
    ```bash
    python services/vision_analysis/client.py
    ```
    A window will appear showing the live camera feed with real-time segmentation and curvature analysis.

### 4. The Autonomous MLOps Loop

This loop allows you to detect and fix model performance drift automatically.

1.  **Detect Drift:** After running the service for a while, analyze the generated logs:
    ```bash
    python scripts/monitoring/drift_detector.py
    ```
    The script will print its findings and save a visual report to `reports/drift_report.png`.

2.  **Trigger Retraining:** If drift is detected, run the automated retraining pipeline:
    ```bash
    python workflows/retraining_pipeline.py
    ```
    This pipeline retrains the model, registers the new version, and promotes it to "staging", completing the autonomous cycle.

### Appendix: Recompiling Protobuf Files

You only need to do this if you modify the `protos/vision.proto` file.

1.  **Navigate to the project root directory.**
2.  **Run the compiler command:**
    ```bash
    python -m grpc_tools.protoc -I./protos --python_out=./pkg/protos --grpc_python_out=./pkg/protos ./protos/vision.proto
    
